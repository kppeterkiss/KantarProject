{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-03T13:27:06.237187Z",
     "start_time": "2024-02-03T13:27:06.192906Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "result_dir = './results/'\n",
    "data_dir = '../KantarData/'\n",
    "\n",
    "figure_dir = \"./figures/\"\n",
    "#os.listdir(data_dir)\n",
    "\n",
    "test=True\n",
    "local_test = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../KantarData/Explicacion_VariablesV3.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m read_able_columns \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mExplicacion_VariablesV3.xlsx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mopenpyxl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./test_data/\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/kantar_data_env2/lib/python3.11/site-packages/pandas/io/excel/_base.py:495\u001B[0m, in \u001B[0;36mread_excel\u001B[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[0m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[1;32m    494\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 495\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    503\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    505\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/kantar_data_env2/lib/python3.11/site-packages/pandas/io/excel/_base.py:1567\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[0m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine \u001B[38;5;241m=\u001B[39m engine\n\u001B[1;32m   1565\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options \u001B[38;5;241m=\u001B[39m storage_options\n\u001B[0;32m-> 1567\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engines\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1568\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_io\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1571\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/kantar_data_env2/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:553\u001B[0m, in \u001B[0;36mOpenpyxlReader.__init__\u001B[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001B[0m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;124;03mReader using openpyxl engine.\u001B[39;00m\n\u001B[1;32m    543\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;124;03m    Arbitrary keyword arguments passed to excel engine.\u001B[39;00m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    552\u001B[0m import_optional_dependency(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenpyxl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 553\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/kantar_data_env2/lib/python3.11/site-packages/pandas/io/excel/_base.py:563\u001B[0m, in \u001B[0;36mBaseExcelReader.__init__\u001B[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m IOHandles(\n\u001B[1;32m    560\u001B[0m     handle\u001B[38;5;241m=\u001B[39mfilepath_or_buffer, compression\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmethod\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m}\n\u001B[1;32m    561\u001B[0m )\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filepath_or_buffer, (ExcelFile, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workbook_class)):\n\u001B[0;32m--> 563\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workbook_class):\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/anaconda3/envs/kantar_data_env2/lib/python3.11/site-packages/pandas/io/common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../KantarData/Explicacion_VariablesV3.xlsx'"
     ]
    }
   ],
   "source": [
    "read_able_columns = pd.read_excel(data_dir+'Explicacion_VariablesV3.xlsx', engine='openpyxl')\n",
    "data_dir = './test_data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T13:27:07.727608Z",
     "start_time": "2024-02-03T13:27:06.245911Z"
    }
   },
   "id": "da4be18d9c231d05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.726079Z"
    }
   },
   "id": "ee6d89a15d1f2dd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#codevf vs coproduct??\n",
    "product_attributes=['CodProducto','CodigoBarras','CodVF', 'VF', 'SA7_BPL', 'SA4_Fabricante', 'SA3_Marca',       'SA2_Submarca',\n",
    "       'X102_Aditivos', 'X108_Usuario', 'X109_Estado', 'X112_Niv_Calorias',\n",
    "       'X124_Estilo', 'X127_Tipo_Bañado_Choc', 'X129_Tipo_Pasteleria',\n",
    "       'X135_Uso', 'X143_Tipo_Corte', 'X163_Info_Cafeina',\n",
    "       'X167_Grado_Curacion', 'X169_Cont_Materia_Grasa', 'X174_Despiece.Corte',\n",
    "       'X182_Tipo_Queso', 'X188_Tipo_Pan', 'X189_Tipo_Viena',\n",
    "       'X206_Tipo_Pescado', 'X230_Niv_Concentracion',\n",
    "       'X314_Tipo_Cena.Entrante', 'X315_Tipo_Carne', 'X323_Tipo_Sazonamiento',\n",
    "       'X328_Tipo_Aceite', 'X329_Pureness_Niveau', 'X360_Tipo_Verdura',\n",
    "       'X366_Tipo_Postre', 'X383_Tipo_Yogurt', 'X384_Tipo_Cena',\n",
    "       'X489_Como_Compro.', 'X490_Fresca.Congelada', 'X491_Presentacion',\n",
    "       'X497_Tipo_Queso', 'X498_Marcas_Queso', 'X500_Tipo_Embutido',\n",
    "       'X514_Tipo_Carne', 'X527_Tipo_Cafe', 'X529_Tipo_Producto',\n",
    "       'X531_Info_Biologica', 'X558_Variedad_Queso', 'X571_Peso_Bebe',\n",
    "       'X674_Tipo_Helado', 'X901_Localizacion', 'X902_Tipo_Producto',\n",
    "       'X903_Envase', 'X904_Variedad.Sabor', 'X5_Num_Tot_Unidades',\n",
    "       'X18_Num_Tot_Paquetes.Botes', 'X66_Preferred_Unit_Measure',]\n",
    "\n",
    "purchase_attributes=['idCompra', 'CodPanelista','CodigoBarras', 'CodProducto','FechaCesta',\t'PrecioCompra', 'Cantidad', 'Precio', 'CodLugarCompra','CodIndividuo','TipoMaquina', 'Promo_Folleto', 'Promo_Envase', 'Promo_TPR','promo']\n",
    "\n",
    "shop_attributes=['CodLugarCompra','Lugar_Compra','Canal','CodIndividuo']\n",
    "\n",
    "\n",
    "customer_attributes=['CodPanelista', 'CC_AA', 'Edad_Ama', 'NF', 'Numero_Gatos','Numero_Perros', 'Presencia_Niños', 'CodIMC', 'IMC', 'Ponderacion','Ciclo_Vida', 'Clase_EGM', 'Clase_Social', 'Habitat_Metropolitano',\n",
    "       'Habitat_Municipal_Std', 'Inmigrante', 'Provincia', 'Region',]\n",
    "\n",
    "tables={'customers': customer_attributes,'products': product_attributes,'shops':shop_attributes,'purchases':purchase_attributes }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.732125Z"
    }
   },
   "id": "a4a23ab0d542f64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define mapping of types\n",
    "type_dict = dict(zip(read_able_columns['Variable'].str.strip(), read_able_columns['Tipo Variable'].str.strip()))\n",
    "\n",
    "\n",
    "type_translation = {'ID': 'int64', 'Fecha': 'datetime', 'Numérica': 'float64', 'Categórica': 'category',\n",
    "                    'Dicotómica': 'bool'}\n",
    "type_dict_new = {k: type_translation[v] for k, v in type_dict.items()}\n",
    "type_dict_new = {('X' + k if k[0].isdigit() else k): v for k, v in type_dict_new.items()}\n",
    "type_dict_new['CodigoBarras'] = 'str'\n",
    "# TODO categorical vs int vs string...\n",
    "# TODO check why it is needed , leave as categorical now..\n",
    "type_dict_new['Presencia_Niños'] = 'category'\n",
    "\n",
    "# when reading, should be given... \n",
    "del type_dict_new['FechaCesta']\n",
    "# because stata cannot handle too many categories with long names:\n",
    "for cn in product_attributes:\n",
    "    if type_dict_new[cn] == 'category':\n",
    "        type_dict_new[cn] = 'str'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T13:27:07.737746Z",
     "start_time": "2024-02-03T13:27:07.736093Z"
    }
   },
   "id": "89d64599806ca140"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def general_model(df):\n",
    "    if df.shape[0]>30:\n",
    "            # for throwing out outliers iteratively if we like, to have a better regression  \n",
    "            cleaning_rounds=1\n",
    "            Y = np.array(df['price_per_unit'])\n",
    "            X = np.array(df['t'])\n",
    "            #flag=np.zeros(y.size)\n",
    "            idx=df[['price_per_unit']].index.copy()\n",
    "            x=X.copy()\n",
    "            y=Y.copy()\n",
    "            print(x)\n",
    "            for i in range(cleaning_rounds):\n",
    "                p = np.polyfit(x, y, deg=5)\n",
    "                ps = np.polyval(p, x)\n",
    "                diff = abs(y-ps)\n",
    "                std=np.std(diff)\n",
    "                mean=np.mean(diff)\n",
    "                good =  abs(diff-mean)/std < 3 \n",
    "                #good = abs(y - ps) < 1  # Here we will only remove positive outliers\n",
    "                    \n",
    "                x_bad, y_bad = x[~good], y[~good]\n",
    "                x, y = x[good], y[good]\n",
    "        \n",
    "           \n",
    "        \n",
    "                if (~good).sum() == 0:\n",
    "                    break\n",
    "            ps = np.polyval(p, X)\n",
    "            diff = abs(Y-ps)\n",
    "            std=np.std(diff)\n",
    "            mean=np.mean(diff)\n",
    "            z= abs(diff-mean)/std\n",
    "            good =  z < 3 \n",
    "            flag = np.logical_not(good)\n",
    "          \n",
    "            ret= pd.DataFrame(index=idx, data=list(zip(ps, z,flag)), columns=['1','2','3'])\n",
    "    else:\n",
    "        Y = np.array(df['price_per_unit'])\n",
    "        #flag=np.zeros(y.size)\n",
    "        idx=df[['price_per_unit']].index.copy()\n",
    "        y=Y.copy()\n",
    "    \n",
    "        std=np.std(Y)\n",
    "        mean=np.mean(Y)\n",
    "        ps = np.full(y.shape, mean)\n",
    "    \n",
    "        z= abs(Y-mean)/std\n",
    "        good =  z < 3 \n",
    "        flag = np.logical_not(good)\n",
    "      \n",
    "        ret= pd.DataFrame(index=idx, data=list(zip(ps, z,flag)), columns=['1','2','3'])\n",
    "\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_oultiers(odf1):\n",
    "       odf1['t'] = odf1['FechaCesta'].astype('datetime64').dt.dayofyear\n",
    "       # calculate price per unit of sales\n",
    "       odf1['price_per_unit']= odf1['Precio']/odf1['Cantidad']\n",
    "       x=  odf1.groupby(['CodProducto'])[['t', 'price_per_unit']].apply(general_model)\n",
    "       x1 = x.reset_index().set_index('level_1').drop(['CodProducto'], axis=1)\n",
    "       odf1[['predicted', 'z', 'outlier']] = x1\n",
    "       return odf1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T13:27:07.743257Z",
     "start_time": "2024-02-03T13:27:07.738821Z"
    }
   },
   "id": "209cf8a1d42d979d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_able_columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.741971Z"
    }
   },
   "id": "fd772a00acab1890"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import traceback\n",
    "from sqlalchemy import create_engine\n",
    "import codecs\n",
    "import numbers\n",
    "\n",
    "\n",
    "all_products=None\n",
    "all_customers=None\n",
    "all_shops=None\n",
    "\n",
    "input_enc=\"ISO-8859-1\"#'latin_1 '\n",
    "#input_enc= 'utf-16-be'\n",
    "\n",
    "def fix_enc(source_df_test, bad_enc=False):\n",
    "\n",
    "\n",
    "    replace_chars={\"Ã.\": \"ñ\", \"Ã³\":\"ó\",\"Ã\":\"í\"}  \n",
    "    orig_names=source_df_test.columns.to_list()\n",
    "    re_encoded_names = [s.encode(input_enc) for s in orig_names]\n",
    "    if bad_enc:\n",
    "        print(orig_names)\n",
    "        re_encoded_names = orig_names    \n",
    "        #re_encoded_names = [codecs.decode(w.encode(input_enc), \"utf-8\") for w in orig_names]\n",
    "        for k,v in replace_chars.items():\n",
    "            re_encoded_names = [w.replace(k,v) for w in re_encoded_names]\n",
    "    \n",
    "        #codecs.decode(w.encode(input_enc), \"utf-8\").replace(\"Ã.\", \"ñ\")\n",
    "    \n",
    "        #re_encoded_names = [s.decode(\"\") for s in re_encoded_names]\n",
    "        \n",
    "        column_rename = dict(zip(source_df_test.columns.to_list()\n",
    "                                 , re_encoded_names))\n",
    "        \n",
    "        # for messed up column names\n",
    "        #column_rename['promo_envase']='Promo_Envase'\n",
    "        #column_rename['promo_folleto']='Promo_Folleto'\n",
    "        print(column_rename)\n",
    "    \n",
    "                    # print(\"EXC -- \",e,k)\n",
    "            \n",
    "    \n",
    "        #source_df_test= source_df_test.replace(replace_chars)\n",
    "        #column_names=source_df_test.columns\n",
    "        #for k,v in replace_chars.items():\n",
    "        #    column_names=[cn.replace(k,v) for cn in column_names]\n",
    "        #column_rename = dict(zip(source_df_test.columns.to_list()\n",
    "        #                         , column_names))\n",
    "        print(source_df_test.columns)\n",
    "\n",
    "        source_df_test = source_df_test.rename(columns=column_rename)\n",
    "        print(source_df_test.columns)\n",
    "    \n",
    "   \n",
    "    for col in source_df_test.columns:\n",
    "        print(f\"-----------processing {col}-----------\")\n",
    "        if col not in type_dict_new:\n",
    "            print('skipping ',col)\n",
    "            continue # for FechaCesta\n",
    "        elem = source_df_test[col].iloc[0]\n",
    "        if source_df_test[col].dtype == \"object\" and ((type_dict_new[col]==\"str\") or (type_dict_new[col]==\"category\")):\n",
    "            source_df_test[col] = source_df_test[col].fillna(\"\")\n",
    "            if isinstance(elem, numbers.Number):\n",
    "                print('Numeric, skipping transform: ',elem)\n",
    "                continue\n",
    "            print('Trying transformation: ',elem)\n",
    "            if bad_enc:\n",
    "                try:\n",
    "                    print(\"String  \",col,' t: ', source_df_test[col].dtype)\n",
    "                    source_df_test[col]=source_df_test[col].str.encode(input_enc)\n",
    "                    source_df_test[col]=source_df_test[col].str.decode('utf-8')\n",
    "                    for k,v in replace_chars.items():\n",
    "                        source_df_test[col] = [w.replace(k,v) for w in source_df_test[col]] \n",
    "                except Exception as e:\n",
    "                    print(\"EXC -- \",e,k)\n",
    "                    traceback.print_exc()           \n",
    "        print(\" Converting \",col ,' -> ',type_dict_new[col] )\n",
    "        if type_dict_new[col]=='float64':\n",
    "            \n",
    "            print('Example element:',elem,type(elem))\n",
    "            if type(elem)==str:\n",
    "                print(col,\" -> removing ','\" )\n",
    "                source_df_test[col] = source_df_test[col].str.replace(\",\",\".\") \n",
    "        source_df_test[col] = source_df_test[col].astype(type_dict_new[col]) \n",
    "        print(col,\" -- \",source_df_test[col].dtype )\n",
    "\n",
    "    \n",
    "\n",
    "    return source_df_test\n",
    "\n",
    "def tell_me_about(s): return (type(s), r'{}'.format(s))\n",
    "\n",
    "def load_and_preprocess_data(year, nrows=None):\n",
    "    # load data\n",
    "    \n",
    "    if year<2018: \n",
    "        bad_enc=True\n",
    "        input_fn = f'{year} data.csv'\n",
    "        delimiter=\",\"\n",
    "        source_df_test=pd.read_csv(data_dir+input_fn,\n",
    "                    encoding=input_enc,\n",
    "                    delimiter=delimiter ,#decimal=decimal,\n",
    "                               on_bad_lines='skip',nrows=nrows,  parse_dates=['FechaCesta'])\n",
    "\n",
    "    else: \n",
    "        bad_enc=False\n",
    "        input_fn=f'Datos_{year}.csv'\n",
    "        delimiter=\";\"\n",
    "        source_df_test=pd.read_csv(data_dir+input_fn,\n",
    "                    encoding=input_enc,\n",
    "                    delimiter=delimiter ,\n",
    "                               on_bad_lines='skip',nrows=nrows,  parse_dates=['FechaCesta'])\n",
    "\n",
    "      \n",
    "    \n",
    "    source_df_test=fix_enc(source_df_test,bad_enc)\n",
    "\n",
    "    # transform \n",
    "    #if nrows:\n",
    "    #    print(source_df_test.iloc[0][['idCompra','Promo_Envase','Promo_Folleto','Promo_TPR']])\n",
    "    \n",
    "    source_df_test['Promo_Envase'] = source_df_test['Promo_Envase'].map({'No': False, 'Si': True})\n",
    "    source_df_test['Promo_Folleto'] = source_df_test['Promo_Folleto'].map({'No': False, 'Si': True})\n",
    "    source_df_test['Promo_TPR'] = source_df_test['Promo_TPR'].map({'No': False, 'Si': True})\n",
    "    \n",
    "    #if nrows:\n",
    "\n",
    "    #    print(source_df_test.iloc[0][['idCompra','Promo_Envase','Promo_Folleto','Promo_TPR']])\n",
    "\n",
    "    '''\n",
    "    for k in type_dict_new.keys():\n",
    "       if k not in source_df_test.columns:\n",
    "           print(source_df_test.columns)\n",
    "           print(k)\n",
    "    '''\n",
    "    #source_df_test = source_df_test.astype(type_dict_new)\n",
    "    \n",
    "    \"\"\"\n",
    "    for k,v in type_dict_new.items():\n",
    "            try:\n",
    "                 #if v==\"categorical\" or v==\"object\":\n",
    "                    #source_df_test[k] = source_df_test[k].astype('str')#.str.encode(\"utf-8\")\n",
    "                    source_df_test[k] = source_df_test[k].astype(v)\n",
    "                    '''\n",
    "                    if k == \"CC_AA\":\n",
    "                        print('CC_AA --- ',v)\n",
    "                        print('str --- ',source_df_test[k].iloc[:3])\n",
    "                        w = source_df_test[k].iloc[1]\n",
    "                       \n",
    "                        print(w,' -> ',codecs.decode(w.encode(\"ISO-8859-1\"),'utf-8'))\n",
    "                    '''\n",
    "                \n",
    "\n",
    "                #else:\n",
    "                #    source_df_test[k]=source_df_test[k].astype('str').str.encode(\"utf-8\")\n",
    "                #    if k == \"CC_AA\":\n",
    "                #        print('str --- ',source_df_test[k].iloc[:3])\n",
    "            except Exception as e:\n",
    "                print(\"EXC -- \",repr(e),k,v)\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                #print(source_df_test[k].head())\n",
    "    \"\"\"\n",
    "    \n",
    "    source_df_test['promo'] = np.where(\n",
    "           source_df_test['Promo_Folleto'] | source_df_test['Promo_Envase'] | source_df_test['Promo_TPR'], True, False)\n",
    "    \n",
    "    return source_df_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(type_dict_new)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.744698Z"
    }
   },
   "id": "9cf4077743d5743"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json \n",
    "from sqlalchemy_utils import drop_database, database_exists,create_database\n",
    "if local_test:\n",
    "    return\n",
    "\n",
    "f=open(\"dbconnection.json\")\n",
    "connection_data = json.load(f)\n",
    "DB_USER =connection_data[\"DB_USER\"]\n",
    "\n",
    "DB_PASS =connection_data[\"DB_PASS\"]\n",
    "DB_HOST =connection_data[\"DB_HOST\"]\n",
    "DB_PORT =connection_data[\"DB_PORT\"]\n",
    "DATABASE =connection_data[\"DATABASE\"]\n",
    "CHARSET =\"utf-8\"\n",
    "\n",
    "   \n",
    "\n",
    "connect_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}?charset={}'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE,CHARSET)\n",
    "connect_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE)\n",
    "\n",
    "\n",
    "if database_exists(connect_string):\n",
    "    drop_database(connect_string)\n",
    "create_database(connect_string)\n",
    "\n",
    "engine = create_engine(connect_string, connect_args={'client_encoding':CHARSET})\n",
    "\n",
    "def to_table(name, df,engine,mode='append'):\n",
    "    from sqlalchemy import create_engine\n",
    "    df.to_sql(name, engine, if_exists=mode,method = 'multi', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.752504Z"
    }
   },
   "id": "9a3b1731a85f32e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def separate_and_save(df, dfs,engine):\n",
    "   # separating the tables:\n",
    "    for name, attributes in tables.items():\n",
    "  \n",
    "        data = df[attributes]\n",
    "        if name == 'purchases':\n",
    "            if not local_test:\n",
    "                to_table(name,data,engine)\n",
    "            del df\n",
    "            #data = get_oultiers(data)     \n",
    "        else:\n",
    "            data = data.drop_duplicates()\n",
    "            dfs[name].append(data)\n",
    "    \n",
    "        #fn = result_dir+f'{year}_{name}.dta'\n",
    "        #dfs[name].append(fn)\n",
    "        \n",
    "           \n",
    "           \n",
    "        #to_table(name,data)\n",
    "        #all_data = pd.concat(df_list, ignore_index=True)\n",
    "   \n",
    "    return dfs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.755363Z"
    }
   },
   "id": "ec76cd37d6ba222b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "dfs={name:[] for name in tables.keys()}\n",
    "nrows=None\n",
    "if test:\n",
    "    nrows=10\n",
    " \n",
    "# first separate and save \n",
    "years =[]\n",
    "for f in os.listdir(data_dir):\n",
    "     if fnmatch.fnmatch(f, 'Datos_*.csv'):\n",
    "        print(f)\n",
    "        try:\n",
    "            year = int(f.replace(\"Datos_\",\"\").replace(\".csv\",\"\"))\n",
    "            years.append(year)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Bad filename {f}: ´{e}\")\n",
    "        \n",
    "        \n",
    "     if fnmatch.fnmatch(f, '* data.csv'):\n",
    "        print(f)\n",
    "        try:\n",
    "            year = int(f.replace(\" data.csv\",\"\"))\n",
    "            years.append(year)\n",
    "        except Exception as e:\n",
    "            print(f\"Bad filename {f}: ´{e}\")\n",
    "\n",
    "for year in years:\n",
    "    print(year)\n",
    "    df= load_and_preprocess_data(year,nrows)\n",
    "    #df=get_oultiers(df)\n",
    "    if local_test:\n",
    "        engine = None\n",
    "    dfs = separate_and_save(df,dfs, engine)\n",
    "    \n",
    "    \n",
    "    for name, l in dfs.items():\n",
    "        if name=='purchases':\n",
    "            continue\n",
    "        data = pd.concat(l, ignore_index=True).drop_duplicates()\n",
    "        if not local_test:\n",
    "            to_table(name,data,engine,\"append\")   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-03T13:27:07.759043Z"
    }
   },
   "id": "f197bc551e9e0290"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
